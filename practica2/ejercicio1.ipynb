{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1\n",
    "\n",
    "Codifique el algoritmo de entrenamiento para una red neuronal con RBF y pruebe su implementación en la resolución del problema `xor`, empleando los datos de la guía anterior.\n",
    "Diseñe una red RBF para resolver el problema Iris (`irisbin.csv`), considerando una cantidad de parámetros similar a la red MLP de la guía anterior. \n",
    "Luego compare la velocidad de entrenamiento y el porcentaje final de clasificación obtenido con ambas arquitecturas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_particiones(filename, n, p_test):\n",
    "    ds = np.genfromtxt(filename, delimiter=',')\n",
    "    n_test = int(ds.shape[0] * p_test)\n",
    "    n_train = ds.shape[0] - n_test\n",
    "    M_test = np.zeros((n,n_test),dtype = int)\n",
    "    M_train = np.zeros((n, n_train),dtype = int)\n",
    "\n",
    "    for i in range(n):\n",
    "        idx=np.random.choice(range(ds.shape[0]),ds.shape[0],replace = False)\n",
    "        idx_test = idx[0:n_test]\n",
    "        idx_train = idx[n_test:ds.shape[0]]\n",
    "        M_test[i] = idx_test\n",
    "        M_train[i] = idx_train\n",
    "    \n",
    "    return (ds, M_test, M_train)\n",
    "\n",
    "def sigmoidea(x):\n",
    "    return np.divide(2, (1 + np.exp(-1 * x))) - 1\n",
    "\n",
    "class layer:\n",
    "    def __init__(self, NNeurons, NInputs):\n",
    "        # Almacenar la dimensión de entrada y cantidad de neuronas\n",
    "        self.neurons_ = NNeurons\n",
    "        self.inputs_ = NInputs\n",
    "\n",
    "        # Inicializar la matriz de pesos de N x M con valores aleatorios\n",
    "        # con una distribución normal centrada en 0 y norma < 0.5 aprox \n",
    "        # self.w = np.random.normal(loc = 0, scale = 0.15, size = (NNeurons, NInputs))\n",
    "        self.w = rng.random((NNeurons, NInputs))\n",
    "        \n",
    "    def eval(self, x):\n",
    "        # Comprobar que la dimension de la entrada es igual a la incializada \n",
    "        assert x.shape[0] == self.inputs_, \\\n",
    "            f\"La entrada de dimensión {x.shape[0]} no coincide con la declarada {self.inputs_}\"\n",
    "\n",
    "        # Producto interno entre la entrada y los pesos\n",
    "        y = np.dot(self.w, x)\n",
    "        \n",
    "        # No linealidad\n",
    "        z = sigmoidea(y)\n",
    "        return z\n",
    "\n",
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, neuronsPerLayer, NInputs):\n",
    "        self.neuronsPerLayer_ = neuronsPerLayer\n",
    "        self.NInputs = NInputs\n",
    "\n",
    "        # La red estará representada como un arreglo de capas\n",
    "        self.network_ = []\n",
    "        \n",
    "        # Auxiliar para definiar la cantidad de entradas de cada capa\n",
    "        # La primera coincide con la entrada de la red\n",
    "        NInputs_aux = NInputs\n",
    "\n",
    "        # Para cada capa representada como el número de neuronas\n",
    "        for layerNeurons in neuronsPerLayer:\n",
    "            # Se crea una capa en base a la cantidad de salidas de la capa\n",
    "            # anterior + 1 (el bias) y con el número de neuronas indicado\n",
    "            self.network_.append(layer(layerNeurons, NInputs_aux + 1))\n",
    "            \n",
    "            # Adelantar la cantidad de entradas para la próxima capa\n",
    "            NInputs_aux = layerNeurons\n",
    "\n",
    "        # Arreglo auxiliar para almacenar los gradientes instantáneos\n",
    "        self.grad_ = []\n",
    "    \n",
    "    def eval(self, input):\n",
    "        # Comprobar que la dimension de la entrada es igual a la incializada \n",
    "        assert input.shape[0] == self.NInputs, \\\n",
    "            f\"La entrada de dimensión {input.shape[0]} no coincide con la declarada {self.NInputs}\"\n",
    "\n",
    "        # La salida de cada capa será acumulada en un arreglo\n",
    "        # que es devuelto para luego utilizar en la etapa de train\n",
    "        y = [input]\n",
    "        \n",
    "        for i in range(len(self.network_)): # Para cada capa en la red\n",
    "            # Agregar el bias a la entrada de la capa i (como primer componente)\n",
    "            x_ = np.hstack((-1, y[i]))\n",
    "\n",
    "            # Calcular la salida de la capa i\n",
    "            y_ = self.network_[i].eval(x_)\n",
    "            \n",
    "            # Agregar la salida al arreglo, que será la entrada de la siguiente\n",
    "            y.append(y_)\n",
    "        \n",
    "        # Devolver la salida como tal, y las salidas intermedias de cada capa\n",
    "        return (y[-1], y)\n",
    "\n",
    "    def backward(self, y, yd):\n",
    "        # Comprobar que la dimension de la salida calculada es igual a la de la deseada \n",
    "        assert yd.shape[0] == y[-1].shape[0], \\\n",
    "            f\"La dimensión de la salida deseada ({yd.shape[0]}) no coincide con la calculada ({y[-1].shape[0]})\"\n",
    "        \n",
    "        # Comprobar que la dimension de la salida deseada es igual a la cantidad de neuronas de salida \n",
    "        assert yd.shape[0] == self.neuronsPerLayer_[-1], \\\n",
    "            f\"La dimensión de la salida deseada ({yd.shape[0]}) no coincide con la capa de salida ({self.neuronsPerLayer_[-1]})\"\n",
    "\n",
    "        # Se calcula el error entre la salida de la red (último componente de y)\n",
    "        # y la salida deseada (la dimensión será la cantidad de neuronas a la salida)\n",
    "        error_ = yd - y[-1]\n",
    "\n",
    "        # Reiniciar el vector de gradientes instantaneos\n",
    "        self.grad_ = []\n",
    "\n",
    "        # Calcular el gradiente de la capa de salida y guardarlo\n",
    "        self.grad_.append((np.multiply(error_, np.multiply((1 + y[-1]), (1 - y[-1])))) * 0.5)\n",
    "        \n",
    "        # Recorriendo las capas desde la penultima hacia la de entrada\n",
    "        for i in range(len(self.network_)-1,0,-1):\n",
    "            # De la capa siguiente (en el orden forward), tomar la matriz de pesos\n",
    "            # sin la columna de pesos asociados al bias, y transponerla (wT_)\n",
    "            wT_ = self.network_[i].w[:,1:].T\n",
    "\n",
    "            # Calcular el gradiente local instantaneo como el producto interno entre\n",
    "            # wT_ y el gradiente de error local de esa misma capa (la siguiente en orden forward)\n",
    "            # en lo que se conoce como retropropagación del error\n",
    "            # TODO: chequear el indexado\n",
    "            d_ = np.dot(wT_, self.grad_[len(self.network_)-1-i])\n",
    "\n",
    "            # Luego multiplicar por la derivada de la sigmoidea\n",
    "            g_ = (np.multiply(d_, np.multiply((1 + y[i]), (1 - y[i])))) * 0.5\n",
    "            \n",
    "            # Agregar al arreglo de gradientes\n",
    "            self.grad_.append(g_)\n",
    "\n",
    "    def update(self, y, lr):\n",
    "        # Actualización de pesos para cada capa\n",
    "        for i in range(len(self.network_)):\n",
    "            # Se calcula el producto entre el gradiente local instantáneo de la capa \n",
    "            # con la entrada de la capa (con bias), esto multiplicado por la tasa\n",
    "            # de aprendizaje resulta en la matriz de actualización de pesos\n",
    "            Dw_ = lr * np.outer(self.grad_[-(i+1)], np.hstack((-1, y[i])))\n",
    "            \n",
    "            # Los nuevos pesos se calculan como el Delta + los pesos \"viejos\"\n",
    "            self.network_[i].w = np.add(self.network_[i].w, Dw_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeans(x, k):\n",
    "    # k define la cantidad de grupos\n",
    "    # se toman k grupos y se le asigna a cada patrón un grupo aleatorio\n",
    "    group_idx = np.random.randint(0, k, x.shape[0])\n",
    "    vec_reasig = np.zeros(x.shape[0])\n",
    "    cant_it = 0\n",
    "    means_vec = np.zeros((k, 2))\n",
    "    stdev_vec = np.zeros((k, 2))\n",
    "\n",
    "    # mientras se hayan hecho reasignaciones (sale cuando todos son verdaderos -> no hubo reasignaciones)\n",
    "    while vec_reasig.all() != True:\n",
    "        cant_it += 1\n",
    "        \n",
    "        # se calculan los centroides de cada grupo\n",
    "        for i in range(k):\n",
    "            group = x[group_idx == i]\n",
    "            # si hay algo en el grupo\n",
    "            if(group.shape[0] != 0):\n",
    "                means_vec[i] =  np.mean(group, axis = 0)\n",
    "                stdev_vec[i] = np.std(group, axis = 0)\n",
    "            # si no hay nada en el grupo, [debe mantener el centroide anterior?]\n",
    "        \n",
    "        # por cada patron\n",
    "        for i, pattern in enumerate(x):\n",
    "            dist_vec = []\n",
    "            for i_group in range(k):\n",
    "                # vector distancias entre el patron y los centroides\n",
    "                dist_vec.append(np.linalg.norm(pattern - means_vec[i_group]))\n",
    "            # índice del centroide que tiene menor distancia\n",
    "            idx_min = np.argmin(dist_vec)\n",
    "            # reasignación de grupo si es necesario\n",
    "            if (group_idx[i] != idx_min):\n",
    "                vec_reasig[i] == False\n",
    "                group_idx[i] = idx_min\n",
    "            # si no hay reasignación se pone en verdadero\n",
    "            else:\n",
    "                vec_reasig[i] = True\n",
    "            \n",
    "    return group_idx, stdev_vec, means_vec, cant_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_gaussian(x, c, mu):\n",
    "    y = np.exp(np.linalg.norm(x - c)) / (2 * mu * mu)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds, testPartitionsIdx, trainPartitionsIdx  =  generar_particiones('gtp2datos/XOR_trn.csv', 1, 0.2)\n",
    "\n",
    "# Separar patrones para validación\n",
    "# Cantidad de patrones para entrenamiento\n",
    "NPatternsTrain = trainPartitionsIdx.shape[1]\n",
    "# Cantidad de patrones a separar para validación\n",
    "NPatternsValidation = 100\n",
    "\n",
    "# Generar el vector de entradas para entrenamiento y validación (solo una partición)\n",
    "X_val = ds[trainPartitionsIdx[0, :NPatternsValidation], :-1]\n",
    "X = ds[trainPartitionsIdx[0, NPatternsValidation:], :-1]\n",
    "# Generar el vector de salida deseada para entrenamiento y validación\n",
    "Yd_val = ds[trainPartitionsIdx[0, :NPatternsValidation], -1]\n",
    "Yd = ds[trainPartitionsIdx[0, NPatternsValidation:], -1].reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5               # cantidad de grupos / RBF\n",
    "\n",
    "# [puede ser que queden grupos vacíos?]\n",
    "idx_groups, std_vec, means_vec, cant_it = kMeans(X, k)\n",
    "\n",
    "# salidas de las RBF -> entradas del mlp\n",
    "mlp_in = np.zeros(X.shape[0])\n",
    "\n",
    "# desvío fijo, igual al promedio entre el desvío de cada dimensión\n",
    "mu_vec = np.mean(std_vec, axis = 1)\n",
    "\n",
    "# por cada patrón, se calcula la salida de la capa radial pasando por la RBF\n",
    "for i, pattern in enumerate(X):\n",
    "    # índice del grupo donde está el patrón\n",
    "    idx_group = idx_groups[i]\n",
    "    \n",
    "    # calculo la salida como la gaussiana con el patrón,\n",
    "    # el centroide de ese grupo, y el desvío de ese grupo\n",
    "\n",
    "    mlp_in[i] = f_gaussian(pattern, means_vec[idx_group], mu_vec[idx_group])\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de acá hay que revisar porque tengo la salida intermedia para cada patrón pero no entiendo cómo hago para convertir eso en las entradas del perceptrón, o si estoy calculando mal la salida de la gaussiana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prueba con perceptrón simple\n",
    "layerStack = np.array([1])\n",
    "perceptron_simple = MultiLayerPerceptron(layerStack, 1)\n",
    "\n",
    "# Parámetros de entrenamiento\n",
    "NEpoch = 1000            # Cantidad de épocas máximas (anterior 2000)\n",
    "errorThr = 0.002         # Umbral de error para finalizar (anterior 0.005)\n",
    "lr = 5E-3                # Tasa de aprendizaje (anterior 8E-3)\n",
    "\n",
    "mlp_in.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jimen\\Documents\\FICH\\2022\\Inteligencia Computacional\\repo_IC\\IC_2022\\practica2\\ejercicio1.ipynb Celda 9\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jimen/Documents/FICH/2022/Inteligencia%20Computacional/repo_IC/IC_2022/practica2/ejercicio1.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NEpoch): \u001b[39m# Para cada época\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jimen/Documents/FICH/2022/Inteligencia%20Computacional/repo_IC/IC_2022/practica2/ejercicio1.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mfor\u001b[39;00m pattern, yd \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(mlp_in, Yd): \u001b[39m# Para cada patrón en la partición\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jimen/Documents/FICH/2022/Inteligencia%20Computacional/repo_IC/IC_2022/practica2/ejercicio1.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m# Calcular la salida según los pesos actuales (pasada hacia adelante)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jimen/Documents/FICH/2022/Inteligencia%20Computacional/repo_IC/IC_2022/practica2/ejercicio1.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         (_, y_) \u001b[39m=\u001b[39m perceptron_simple\u001b[39m.\u001b[39;49meval(pattern)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jimen/Documents/FICH/2022/Inteligencia%20Computacional/repo_IC/IC_2022/practica2/ejercicio1.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m#print(f'salida{i}={y}')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jimen/Documents/FICH/2022/Inteligencia%20Computacional/repo_IC/IC_2022/practica2/ejercicio1.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jimen/Documents/FICH/2022/Inteligencia%20Computacional/repo_IC/IC_2022/practica2/ejercicio1.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39m# Realizar la propagación hacia atrás donde se calculan los gradientes\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jimen/Documents/FICH/2022/Inteligencia%20Computacional/repo_IC/IC_2022/practica2/ejercicio1.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39m# instantáneos (pasada hacia atrás)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jimen/Documents/FICH/2022/Inteligencia%20Computacional/repo_IC/IC_2022/practica2/ejercicio1.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         grad_ \u001b[39m=\u001b[39m perceptron_simple\u001b[39m.\u001b[39mbackward(y_, yd)\n",
      "\u001b[1;32mc:\\Users\\jimen\\Documents\\FICH\\2022\\Inteligencia Computacional\\repo_IC\\IC_2022\\practica2\\ejercicio1.ipynb Celda 9\u001b[0m in \u001b[0;36mMultiLayerPerceptron.eval\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jimen/Documents/FICH/2022/Inteligencia%20Computacional/repo_IC/IC_2022/practica2/ejercicio1.ipynb#X14sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meval\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jimen/Documents/FICH/2022/Inteligencia%20Computacional/repo_IC/IC_2022/practica2/ejercicio1.ipynb#X14sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     \u001b[39m# Comprobar que la dimension de la entrada es igual a la incializada \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jimen/Documents/FICH/2022/Inteligencia%20Computacional/repo_IC/IC_2022/practica2/ejercicio1.ipynb#X14sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m] \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mNInputs, \\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jimen/Documents/FICH/2022/Inteligencia%20Computacional/repo_IC/IC_2022/practica2/ejercicio1.ipynb#X14sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLa entrada de dimensión \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m no coincide con la declarada \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mNInputs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jimen/Documents/FICH/2022/Inteligencia%20Computacional/repo_IC/IC_2022/practica2/ejercicio1.ipynb#X14sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39m# La salida de cada capa será acumulada en un arreglo\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jimen/Documents/FICH/2022/Inteligencia%20Computacional/repo_IC/IC_2022/practica2/ejercicio1.ipynb#X14sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     \u001b[39m# que es devuelto para luego utilizar en la etapa de train\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jimen/Documents/FICH/2022/Inteligencia%20Computacional/repo_IC/IC_2022/practica2/ejercicio1.ipynb#X14sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     y \u001b[39m=\u001b[39m [\u001b[39minput\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# Arreglos auxiliares para guardar la evolución del error de la red\n",
    "errorRatePerEpoch = []\n",
    "MMSEPerEpoch = []\n",
    "\n",
    "for epoch in range(NEpoch): # Para cada época\n",
    "    for pattern, yd in zip(mlp_in, Yd): # Para cada patrón en la partición\n",
    "        # Calcular la salida según los pesos actuales (pasada hacia adelante)\n",
    "        (_, y_) = perceptron_simple.eval(pattern)\n",
    "        #print(f'salida{i}={y}')\n",
    "\n",
    "        # Realizar la propagación hacia atrás donde se calculan los gradientes\n",
    "        # instantáneos (pasada hacia atrás)\n",
    "        grad_ = perceptron_simple.backward(y_, yd)\n",
    "        \n",
    "        # Actualizar los pesos de la red\n",
    "        perceptron_simple.update(y_, lr)\n",
    "    \n",
    "    # Para la validación se utilizarán solo algunos patrones y se calculará una\n",
    "    # tasa de error, si esta es menor al umbral, se termina el proceso de entrenamiento\n",
    "    # A la vez, se calculará el error cuadrático medio para tener una evolución\n",
    "    # de dicha variable a lo largo de las épocas\n",
    "    errorsAccum_ = 0    # Acumulador de errores\n",
    "    SEAcumm_ = 0        # Acumulador error cuadrático\n",
    "\n",
    "    # [valido con todos los patrones]\n",
    "    for patron, yd in zip(mlp_in, Yd):\n",
    "        # Evaluar el patron\n",
    "        (z_, _) = perceptron_simple.eval(patron)\n",
    "        \n",
    "        # Codificación de la salida en las 2 clases\n",
    "        y_ = -1 if (z_ < 0) else 1\n",
    "        \n",
    "        # Comparación con la salida deseada y acumulación de errores\n",
    "        errorsAccum_ += int(y_ != yd)\n",
    "\n",
    "        # Cálculo del error cuadrático y acumulación\n",
    "        SEAcumm_ += np.sum(np.square(yd - z_))\n",
    "\n",
    "    # Tasa de error: errores / patrones evaluados\n",
    "    errorRate_ = (errorsAccum_/X.shape[0])\n",
    "    # Guardar la tasa de error de1000 la época\n",
    "    errorRatePerEpoch= np.append(errorRatePerEpoch, [errorRate_])\n",
    "    \n",
    "    # Calcular el error cuadrático medio promedio: MSE / patrones evaluados\n",
    "    MSEMean_= (SEAcumm_/X.shape[0])\n",
    "    # Guardar el error cuadrático medio promedio de la época\n",
    "    MMSEPerEpoch = np.append(MMSEPerEpoch, MSEMean_)\n",
    "\n",
    "    # Si la tasa de error es menor al umbral, termina el proceso de entrenamiento\n",
    "    if (errorRate_ < errorThr):\n",
    "        break\n",
    "\n",
    "    # Cada 100 épocas mostrar el error\n",
    "    # if (epoch+1) % 100 == 0:\n",
    "    #     print(f'Época {epoch+1}: tasa de error de {errorRatePerEpoch[-1]} | MSE promedio {MMSEPerEpoch[-1]}')\n",
    "   \n",
    "# Imprimir información acerca del entrenamiento\n",
    "print(f'Finalizó en la época {epoch+1} con una tasa de error de {errorRatePerEpoch[-1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07ff7e21ea611955a5ea046bb85e7833bbeb87a9446667deecdccb9fe0872cdd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
