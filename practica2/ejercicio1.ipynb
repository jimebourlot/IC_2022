{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1\n",
    "\n",
    "Codifique el algoritmo de entrenamiento para una red neuronal con RBF y pruebe su implementación en la resolución del problema `xor`, empleando los datos de la guía anterior.\n",
    "Diseñe una red RBF para resolver el problema Iris (`irisbin.csv`), considerando una cantidad de parámetros similar a la red MLP de la guía anterior. \n",
    "Luego compare la velocidad de entrenamiento y el porcentaje final de clasificación obtenido con ambas arquitecturas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_particiones(filename, n, p_test):\n",
    "    ds = np.genfromtxt(filename, delimiter=',')\n",
    "    n_test = int(ds.shape[0] * p_test)\n",
    "    n_train = ds.shape[0] - n_test\n",
    "    M_test = np.zeros((n,n_test),dtype = int)\n",
    "    M_train = np.zeros((n, n_train),dtype = int)\n",
    "\n",
    "    for i in range(n):\n",
    "        idx=np.random.choice(range(ds.shape[0]),ds.shape[0],replace = False)\n",
    "        idx_test = idx[0:n_test]\n",
    "        idx_train = idx[n_test:ds.shape[0]]\n",
    "        M_test[i] = idx_test\n",
    "        M_train[i] = idx_train\n",
    "    \n",
    "    return (ds, M_test, M_train)\n",
    "\n",
    "def sigmoidea(x):\n",
    "    return np.divide(2, (1 + np.exp(-1 * x))) - 1\n",
    "\n",
    "class layer:\n",
    "    def __init__(self, NNeurons, NInputs):\n",
    "        # Almacenar la dimensión de entrada y cantidad de neuronas\n",
    "        self.neurons_ = NNeurons\n",
    "        self.inputs_ = NInputs\n",
    "\n",
    "        # Inicializar la matriz de pesos de N x M con valores aleatorios\n",
    "        # con una distribución normal centrada en 0 y norma < 0.5 aprox \n",
    "        # self.w = np.random.normal(loc = 0, scale = 0.15, size = (NNeurons, NInputs))\n",
    "        self.w = rng.random((NNeurons, NInputs))\n",
    "        \n",
    "    def eval(self, x):\n",
    "        # Comprobar que la dimension de la entrada es igual a la incializada \n",
    "        assert x.shape[0] == self.inputs_, \\\n",
    "            f\"La entrada de dimensión {x.shape[0]} no coincide con la declarada {self.inputs_}\"\n",
    "\n",
    "        # Producto interno entre la entrada y los pesos\n",
    "        y = np.dot(self.w, x)\n",
    "        \n",
    "        # No linealidad\n",
    "        z = sigmoidea(y)\n",
    "        return z\n",
    "\n",
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, neuronsPerLayer, NInputs):\n",
    "        self.neuronsPerLayer_ = neuronsPerLayer\n",
    "        self.NInputs = NInputs\n",
    "\n",
    "        # La red estará representada como un arreglo de capas\n",
    "        self.network_ = []\n",
    "        \n",
    "        # Auxiliar para definiar la cantidad de entradas de cada capa\n",
    "        # La primera coincide con la entrada de la red\n",
    "        NInputs_aux = NInputs\n",
    "\n",
    "        # Para cada capa representada como el número de neuronas\n",
    "        for layerNeurons in neuronsPerLayer:\n",
    "            # Se crea una capa en base a la cantidad de salidas de la capa\n",
    "            # anterior + 1 (el bias) y con el número de neuronas indicado\n",
    "            self.network_.append(layer(layerNeurons, NInputs_aux + 1))\n",
    "            \n",
    "            # Adelantar la cantidad de entradas para la próxima capa\n",
    "            NInputs_aux = layerNeurons\n",
    "\n",
    "        # Arreglo auxiliar para almacenar los gradientes instantáneos\n",
    "        self.grad_ = []\n",
    "    \n",
    "    def eval(self, input):\n",
    "        # Comprobar que la dimension de la entrada es igual a la incializada \n",
    "        assert input.shape[0] == self.NInputs, \\\n",
    "            f\"La entrada de dimensión {input.shape[0]} no coincide con la declarada {self.NInputs}\"\n",
    "\n",
    "        # La salida de cada capa será acumulada en un arreglo\n",
    "        # que es devuelto para luego utilizar en la etapa de train\n",
    "        y = [input]\n",
    "        \n",
    "        for i in range(len(self.network_)): # Para cada capa en la red\n",
    "            # Agregar el bias a la entrada de la capa i (como primer componente)\n",
    "            x_ = np.hstack((-1, y[i]))\n",
    "\n",
    "            # Calcular la salida de la capa i\n",
    "            y_ = self.network_[i].eval(x_)\n",
    "            \n",
    "            # Agregar la salida al arreglo, que será la entrada de la siguiente\n",
    "            y.append(y_)\n",
    "        \n",
    "        # Devolver la salida como tal, y las salidas intermedias de cada capa\n",
    "        return (y[-1], y)\n",
    "\n",
    "    def backward(self, y, yd):\n",
    "        # Comprobar que la dimension de la salida calculada es igual a la de la deseada \n",
    "        assert yd.shape[0] == y[-1].shape[0], \\\n",
    "            f\"La dimensión de la salida deseada ({yd.shape[0]}) no coincide con la calculada ({y[-1].shape[0]})\"\n",
    "        \n",
    "        # Comprobar que la dimension de la salida deseada es igual a la cantidad de neuronas de salida \n",
    "        assert yd.shape[0] == self.neuronsPerLayer_[-1], \\\n",
    "            f\"La dimensión de la salida deseada ({yd.shape[0]}) no coincide con la capa de salida ({self.neuronsPerLayer_[-1]})\"\n",
    "\n",
    "        # Se calcula el error entre la salida de la red (último componente de y)\n",
    "        # y la salida deseada (la dimensión será la cantidad de neuronas a la salida)\n",
    "        error_ = yd - y[-1]\n",
    "\n",
    "        # Reiniciar el vector de gradientes instantaneos\n",
    "        self.grad_ = []\n",
    "\n",
    "        # Calcular el gradiente de la capa de salida y guardarlo\n",
    "        self.grad_.append((np.multiply(error_, np.multiply((1 + y[-1]), (1 - y[-1])))) * 0.5)\n",
    "        \n",
    "        # Recorriendo las capas desde la penultima hacia la de entrada\n",
    "        for i in range(len(self.network_)-1,0,-1):\n",
    "            # De la capa siguiente (en el orden forward), tomar la matriz de pesos\n",
    "            # sin la columna de pesos asociados al bias, y transponerla (wT_)\n",
    "            wT_ = self.network_[i].w[:,1:].T\n",
    "\n",
    "            # Calcular el gradiente local instantaneo como el producto interno entre\n",
    "            # wT_ y el gradiente de error local de esa misma capa (la siguiente en orden forward)\n",
    "            # en lo que se conoce como retropropagación del error\n",
    "            # TODO: chequear el indexado\n",
    "            d_ = np.dot(wT_, self.grad_[len(self.network_)-1-i])\n",
    "\n",
    "            # Luego multiplicar por la derivada de la sigmoidea\n",
    "            g_ = (np.multiply(d_, np.multiply((1 + y[i]), (1 - y[i])))) * 0.5\n",
    "            \n",
    "            # Agregar al arreglo de gradientes\n",
    "            self.grad_.append(g_)\n",
    "\n",
    "    def update(self, y, lr):\n",
    "        # Actualización de pesos para cada capa\n",
    "        for i in range(len(self.network_)):\n",
    "            # Se calcula el producto entre el gradiente local instantáneo de la capa \n",
    "            # con la entrada de la capa (con bias), esto multiplicado por la tasa\n",
    "            # de aprendizaje resulta en la matriz de actualización de pesos\n",
    "            Dw_ = lr * np.outer(self.grad_[-(i+1)], np.hstack((-1, y[i])))\n",
    "            \n",
    "            # Los nuevos pesos se calculan como el Delta + los pesos \"viejos\"\n",
    "            self.network_[i].w = np.add(self.network_[i].w, Dw_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeans(x, k):\n",
    "    # k define la cantidad de grupos\n",
    "    # se toman k grupos y se le asigna a cada patrón un grupo aleatorio\n",
    "    group_idx = np.random.randint(0, k, x.shape[0])\n",
    "    vec_reasig = np.zeros(x.shape[0])\n",
    "    cant_it = 0\n",
    "    means_vec = np.zeros((k, 2))\n",
    "    stdev_vec = np.zeros((k, 2))\n",
    "\n",
    "    # mientras se hayan hecho reasignaciones (sale cuando todos son verdaderos -> no hubo reasignaciones)\n",
    "    while vec_reasig.all() != True:\n",
    "        cant_it += 1\n",
    "        \n",
    "        # se calculan los centroides de cada grupo\n",
    "        for i in range(k):\n",
    "            group = x[group_idx == i]\n",
    "            # si hay algo en el grupo\n",
    "            if(group.shape[0] != 0):\n",
    "                means_vec[i] =  np.mean(group, axis = 0)\n",
    "                stdev_vec[i] = np.std(group, axis = 0)\n",
    "            # si no hay nada en el grupo, [debe mantener el centroide anterior?]\n",
    "        \n",
    "        # por cada patron\n",
    "        for i, pattern in enumerate(x):\n",
    "            dist_vec = []\n",
    "            for i_group in range(k):\n",
    "                # vector distancias entre el patron y los centroides\n",
    "                dist_vec.append(np.linalg.norm(pattern - means_vec[i_group]))\n",
    "            # índice del centroide que tiene menor distancia\n",
    "            idx_min = np.argmin(dist_vec)\n",
    "            # reasignación de grupo si es necesario\n",
    "            if (group_idx[i] != idx_min):\n",
    "                vec_reasig[i] == False\n",
    "                group_idx[i] = idx_min\n",
    "            # si no hay reasignación se pone en verdadero\n",
    "            else:\n",
    "                vec_reasig[i] = True\n",
    "            \n",
    "    return group_idx, stdev_vec, means_vec, cant_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_gaussian(x, c, mu):\n",
    "    d_ = np.subtract(x, c)\n",
    "    c_ = np.diag(1 / mu)\n",
    "    y = np.exp(np.dot(np.dot(d_.T, c_), d_))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds, testPartitionsIdx, trainPartitionsIdx  =  generar_particiones('gtp2datos/XOR_trn.csv', 1, 0.2)\n",
    "\n",
    "# Separar patrones para validación\n",
    "# Cantidad de patrones para entrenamiento\n",
    "NPatternsTrain = trainPartitionsIdx.shape[1]\n",
    "# Cantidad de patrones a separar para validación\n",
    "NPatternsValidation = 100\n",
    "\n",
    "# Generar el vector de entradas para entrenamiento y validación (solo una partición)\n",
    "X_val = ds[trainPartitionsIdx[0, :NPatternsValidation], :-1]\n",
    "X = ds[trainPartitionsIdx[0, NPatternsValidation:], :-1]\n",
    "# Generar el vector de salida deseada para entrenamiento y validación\n",
    "Yd_val = ds[trainPartitionsIdx[0, :NPatternsValidation], -1]\n",
    "Yd = ds[trainPartitionsIdx[0, NPatternsValidation:], -1].reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5               # cantidad de grupos / RBF\n",
    "\n",
    "# [puede ser que queden grupos vacíos?]\n",
    "idx_groups, std_vec, means_vec, cant_it = kMeans(X, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig1 = plt.figure(figsize=(5, 4), tight_layout=True)\n",
    "# fig1.suptitle('Recta $w_1 x + w_2 y - w_0 = 0$',  fontsize=11)\n",
    "fig1_ax = fig1.add_subplot(autoscale_on=False, xlim=(-1.5, 1.5), ylim=(-1.5, 1.5))\n",
    "fig1_ax.set_aspect('equal')\n",
    "fig1_ax.grid()\n",
    "\n",
    "# Patrones\n",
    "x_x = X[:,0]\n",
    "x_y = X[:,1]\n",
    "x_color = np.where(Yd[:,0] == 1, 'C0', 'C2')\n",
    "fig1_ax.scatter(x_x, x_y, c=x_color, s=5, marker='.')\n",
    "\n",
    "# k-means\n",
    "fig1_ax.scatter(means_vec[:,0], means_vec[:,1], marker='x', c='C3')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salidas de las RBF -> entradas del mlp\n",
    "mlp_in = np.zeros((X.shape[0], k))\n",
    "\n",
    "# desvío fijo, igual al promedio entre el desvío de cada dimensión\n",
    "mu_vec = np.mean(std_vec, axis = 1)\n",
    "\n",
    "# por cada patrón, se calcula la salida de la capa radial pasando por la RBF\n",
    "for i, pattern in enumerate(X):\n",
    "    # índice del grupo donde está el patrón\n",
    "    idx_group = idx_groups[i]\n",
    "    \n",
    "    # calculo la salida como la gaussiana con el patrón,\n",
    "    # el centroide de ese grupo, y el desvío de ese grupo\n",
    "\n",
    "    _aux = [f_gaussian(pattern, _mean, _mu) for (_mean, _mu) in zip(means_vec, mu_vec)]\n",
    "    mlp_in[i] = _aux / np.linalg.norm(_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 5)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prueba con perceptrón simple\n",
    "layerStack = np.array([1])\n",
    "perceptron_simple = MultiLayerPerceptron(layerStack, k)\n",
    "\n",
    "# Parámetros de entrenamiento\n",
    "NEpoch = 100            # Cantidad de épocas máximas (anterior 2000)\n",
    "errorThr = 0.002         # Umbral de error para finalizar (anterior 0.005)\n",
    "lr = 5E-1                # Tasa de aprendizaje (anterior 8E-3)\n",
    "\n",
    "mlp_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizó en la época 1 con una tasa de error de 0.0\n"
     ]
    }
   ],
   "source": [
    "# Arreglos auxiliares para guardar la evolución del error de la red\n",
    "errorRatePerEpoch = []\n",
    "MMSEPerEpoch = []\n",
    "\n",
    "for epoch in range(NEpoch): # Para cada época\n",
    "    for pattern, yd in zip(mlp_in, Yd): # Para cada patrón en la partición\n",
    "        # Calcular la salida según los pesos actuales (pasada hacia adelante)\n",
    "        (_, y_) = perceptron_simple.eval(pattern)\n",
    "        #print(f'salida{i}={y}')\n",
    "\n",
    "        # Realizar la propagación hacia atrás donde se calculan los gradientes\n",
    "        # instantáneos (pasada hacia atrás)\n",
    "        perceptron_simple.backward(y_, yd)\n",
    "        \n",
    "        # Actualizar los pesos de la red\n",
    "        perceptron_simple.update(y_, lr)\n",
    "    \n",
    "    # Para la validación se utilizarán solo algunos patrones y se calculará una\n",
    "    # tasa de error, si esta es menor al umbral, se termina el proceso de entrenamiento\n",
    "    # A la vez, se calculará el error cuadrático medio para tener una evolución\n",
    "    # de dicha variable a lo largo de las épocas\n",
    "    errorsAccum_ = 0    # Acumulador de errores\n",
    "    SEAcumm_ = 0        # Acumulador error cuadrático\n",
    "\n",
    "    # [valido con todos los patrones]\n",
    "    for patron, yd in zip(mlp_in, Yd):\n",
    "        # Evaluar el patron\n",
    "        (z_, _) = perceptron_simple.eval(patron)\n",
    "        \n",
    "        # Codificación de la salida en las 2 clases\n",
    "        y_ = -1 if (z_ < 0) else 1\n",
    "        \n",
    "        # Comparación con la salida deseada y acumulación de errores\n",
    "        errorsAccum_ += int(y_ != yd)\n",
    "\n",
    "        # Cálculo del error cuadrático y acumulación\n",
    "        SEAcumm_ += np.sum(np.square(yd - z_))\n",
    "\n",
    "    # Tasa de error: errores / patrones evaluados\n",
    "    errorRate_ = (errorsAccum_/X.shape[0])\n",
    "    # Guardar la tasa de error de1000 la época\n",
    "    errorRatePerEpoch= np.append(errorRatePerEpoch, [errorRate_])\n",
    "    \n",
    "    # Calcular el error cuadrático medio promedio: MSE / patrones evaluados\n",
    "    MSEMean_= (SEAcumm_/X.shape[0])\n",
    "    # Guardar el error cuadrático medio promedio de la época\n",
    "    MMSEPerEpoch = np.append(MMSEPerEpoch, MSEMean_)\n",
    "\n",
    "    # Si la tasa de error es menor al umbral, termina el proceso de entrenamiento\n",
    "    if (errorRate_ < errorThr):\n",
    "        break\n",
    "\n",
    "    # Cada 100 épocas mostrar el error\n",
    "    # if (epoch+1) % 100 == 0:\n",
    "    #     print(f'Época {epoch+1}: tasa de error de {errorRatePerEpoch[-1]} | MSE promedio {MMSEPerEpoch[-1]}')\n",
    "   \n",
    "# Imprimir información acerca del entrenamiento\n",
    "print(f'Finalizó en la época {epoch+1} con una tasa de error de {errorRatePerEpoch[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa de error en prueba 0.0\n"
     ]
    }
   ],
   "source": [
    "# Procesar los datos para test\n",
    "# Generar el vector de entradas para test\n",
    "XTest = ds[testPartitionsIdx[0,:], :-1]\n",
    "# Generar el vector de salida deseada para test\n",
    "YdTest = ds[testPartitionsIdx[0,:], -1]\n",
    "\n",
    "mlp_in_test = np.zeros((XTest.shape[0], k))\n",
    "\n",
    "# por cada patrón, se calcula la salida de la capa radial pasando por la RBF\n",
    "for i, pattern in enumerate(XTest):\n",
    "    _aux = [f_gaussian(pattern, _mean, _mu) for (_mean, _mu) in zip(means_vec, mu_vec)]\n",
    "    mlp_in_test[i] = _aux / np.linalg.norm(_aux)\n",
    "\n",
    "errorsAccum_ = 0     # Acumulador de errores\n",
    "for pattern, yd in zip(mlp_in_test, YdTest):\n",
    "    # Calcular la salida\n",
    "    (z_, _) = perceptron_simple.eval(pattern)\n",
    "    \n",
    "    # Codificación de la salida\n",
    "    y_ = -1 if (z_[-1] < 0) else 1\n",
    "    \n",
    "    # Comparación con la salida deseada y acumulación de errores\n",
    "    errorsAccum_ += int(y_ != yd)\n",
    "\n",
    "print(f'Tasa de error en prueba {errorsAccum_ / XTest.shape[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
