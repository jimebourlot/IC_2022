{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_particiones(filename, n, p_test):\n",
    "    ds = np.genfromtxt(filename, delimiter=',')\n",
    "    n_test = int(ds.shape[0] * p_test)\n",
    "    n_train = ds.shape[0] - n_test\n",
    "    M_test = np.zeros((n,n_test),dtype = int)\n",
    "    M_train = np.zeros((n, n_train),dtype = int)\n",
    "\n",
    "    for i in range(n):\n",
    "        idx=np.random.choice(range(ds.shape[0]),ds.shape[0],replace = False)\n",
    "        idx_test = idx[0:n_test]\n",
    "        idx_train = idx[n_test:ds.shape[0]]\n",
    "        M_test[i] = idx_test\n",
    "        M_train[i] = idx_train\n",
    "    \n",
    "    return (ds, M_test, M_train)\n",
    "\n",
    "def sigmoidea(x):\n",
    "    return np.divide(2, (1 + np.exp(-1 * x))) - 1\n",
    "\n",
    "\n",
    "class layer:\n",
    "    def __init__(self, NNeurons, NInputs):\n",
    "        # Almacenar la dimensión de entrada y cantidad de neuronas\n",
    "        self.neurons_ = NNeurons\n",
    "        self.inputs_ = NInputs\n",
    "\n",
    "        # Inicializar la matriz de pesos de N x M con valores aleatorios\n",
    "        # con una distribución normal centrada en 0 y norma < 0.5 aprox \n",
    "        # self.w = np.random.normal(loc = 0, scale = 0.15, size = (NNeurons, NInputs))\n",
    "        self.w = rng.random((NNeurons, NInputs))\n",
    "        \n",
    "    def eval(self, x):\n",
    "        # Comprobar que la dimension de la entrada es igual a la incializada \n",
    "        assert x.shape[0] == self.inputs_, \\\n",
    "            f\"La entrada de dimensión {x.shape[0]} no coincide con la declarada {self.inputs_}\"\n",
    "\n",
    "        # Producto interno entre la entrada y los pesos\n",
    "        y = np.dot(self.w, x)\n",
    "        \n",
    "        # No linealidad\n",
    "        z = sigmoidea(y)\n",
    "        return y\n",
    "\n",
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, neuronsPerLayer, NInputs):\n",
    "        self.neuronsPerLayer_ = neuronsPerLayer\n",
    "        self.NInputs = NInputs\n",
    "\n",
    "        # La red estará representada como un arreglo de capas\n",
    "        self.network_ = []\n",
    "        \n",
    "        # Auxiliar para definiar la cantidad de entradas de cada capa\n",
    "        # La primera coincide con la entrada de la red\n",
    "        NInputs_aux = NInputs\n",
    "\n",
    "        # Para cada capa representada como el número de neuronas\n",
    "        for layerNeurons in neuronsPerLayer:\n",
    "            # Se crea una capa en base a la cantidad de salidas de la capa\n",
    "            # anterior + 1 (el bias) y con el número de neuronas indicado\n",
    "            self.network_.append(layer(layerNeurons, NInputs_aux + 1))\n",
    "            \n",
    "            # Adelantar la cantidad de entradas para la próxima capa\n",
    "            NInputs_aux = layerNeurons\n",
    "\n",
    "        # Arreglo auxiliar para almacenar los gradientes instantáneos\n",
    "        self.grad_ = []\n",
    "    \n",
    "    def eval(self, input):\n",
    "        # Comprobar que la dimension de la entrada es igual a la incializada \n",
    "        assert input.shape[0] == self.NInputs, \\\n",
    "            f\"La entrada de dimensión {input.shape[0]} no coincide con la declarada {self.NInputs}\"\n",
    "\n",
    "        # La salida de cada capa será acumulada en un arreglo\n",
    "        # que es devuelto para luego utilizar en la etapa de train\n",
    "        y = [input]\n",
    "        \n",
    "        for i in range(len(self.network_)): # Para cada capa en la red\n",
    "            # Agregar el bias a la entrada de la capa i (como primer componente)\n",
    "            x_ = np.hstack((-1, y[i]))\n",
    "\n",
    "            # Calcular la salida de la capa i\n",
    "            y_ = self.network_[i].eval(x_)\n",
    "            \n",
    "            # Agregar la salida al arreglo, que será la entrada de la siguiente\n",
    "            y.append(y_)\n",
    "        \n",
    "        # Devolver la salida como tal, y las salidas intermedias de cada capa\n",
    "        return (y[-1], y)\n",
    "\n",
    "    def backward(self, y, yd):\n",
    "       \n",
    "        # Comprobar que la dimension de la salida calculada es igual a la de la deseada \n",
    "        assert yd.shape[0] == y[-1].shape[0], \\\n",
    "            f\"La dimensión de la salida deseada ({yd.shape[0]}) no coincide con la calculada ({y[-1].shape[0]})\"\n",
    "        \n",
    "        # Comprobar que la dimension de la salida deseada es igual a la cantidad de neuronas de salida \n",
    "        assert yd.shape[0] == self.neuronsPerLayer_[-1], \\\n",
    "            f\"La dimensión de la salida deseada ({yd.shape[0]}) no coincide con la capa de salida ({self.neuronsPerLayer_[-1]})\"\n",
    "\n",
    "        # Se calcula el error entre la salida de la red (último componente de y)\n",
    "        # y la salida deseada (la dimensión será la cantidad de neuronas a la salida)\n",
    "        error_ = yd - y[-1]\n",
    "\n",
    "        # Reiniciar el vector de gradientes instantaneos\n",
    "        self.grad_ = []\n",
    "\n",
    "        # Calcular el gradiente de la capa de salida y guardarlo\n",
    "        self.grad_.append(error_)\n",
    "        # Recorriendo las capas desde la penultima hacia la de entrada\n",
    "        for i in range(len(self.network_)-1,0,-1):\n",
    "            # De la capa siguiente (en el orden forward), tomar la matriz de pesos\n",
    "            # sin la columna de pesos asociados al bias, y transponerla (wT_)\n",
    "            wT_ = self.network_[i].w[:,1:].T\n",
    "\n",
    "            # Calcular el gradiente local instantaneo como el producto interno entre\n",
    "            # wT_ y el gradiente de error local de esa misma capa (la siguiente en orden forward)\n",
    "            # en lo que se conoce como retropropagación del error\n",
    "            # TODO: chequear el indexado\n",
    "            d_ = np.dot(wT_, self.grad_[len(self.network_)-1-i])\n",
    "\n",
    "            # Luego multiplicar por la derivada de la sigmoidea\n",
    "            g_ = (np.multiply(d_, np.multiply((1 + y[i]), (1 - y[i])))) * 0.5\n",
    "            \n",
    "            # Agregar al arreglo de gradientes\n",
    "            self.grad_.append(g_)\n",
    "\n",
    "    def update(self, y, lr):\n",
    "        # Actualización de pesos para cada capa\n",
    "        for i in range(len(self.network_)):\n",
    "            # Se calcula el producto entre el gradiente local instantáneo de la capa \n",
    "            # con la entrada de la capa (con bias), esto multiplicado por la tasa\n",
    "            # de aprendizaje resulta en la matriz de actualización de pesos\n",
    "            Dw_ = lr * np.outer(self.grad_[-(i+1)], np.hstack((-1, y[i])))\n",
    "            # Los nuevos pesos se calculan como el Delta + los pesos \"viejos\"\n",
    "            self.network_[i].w = np.add(self.network_[i].w, Dw_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeans(x, k):\n",
    "    # k define la cantidad de grupos\n",
    "    # se toman k grupos y se le asigna a cada patrón un grupo aleatorio\n",
    "    group_idx = np.repeat(range(k),np.ceil(x.shape[0]/k))[:x.shape[0]]\n",
    "    \n",
    "    \n",
    "        \n",
    "    vec_reasig = np.zeros(x.shape[0])\n",
    "    cant_it = 0\n",
    "    means_vec = np.zeros((k, x.shape[1]))\n",
    "    stdev_vec = np.zeros((k, x.shape[1]))\n",
    "\n",
    "    # mientras se hayan hecho reasignaciones (sale cuando todos son verdaderos -> no hubo reasignaciones)\n",
    "    while vec_reasig.all() != True:\n",
    "        cant_it += 1\n",
    "        \n",
    "        # se calculan los centroides de cada grupo\n",
    "        for i in range(k):\n",
    "            group = x[group_idx == i]\n",
    "            # si hay algo en el grupo\n",
    "            if(group.shape[0] > 0):\n",
    "                means_vec[i] =  np.mean(group, axis = 0)\n",
    "                # if(group.shape[0] > 1):\n",
    "                #     stdev_vec[i] = np.std(group, axis = 0)\n",
    "\n",
    "        \n",
    "        # por cada patron\n",
    "        for i, pattern in enumerate(x):\n",
    "            dist_vec = []\n",
    "            for i_group in range(k):\n",
    "                # vector distancias entre el patron y los centroides\n",
    "                dist_vec.append(np.linalg.norm(pattern - means_vec[i_group]))\n",
    "            # índice del centroide que tiene menor distancia\n",
    "            idx_min = np.argmin(dist_vec)\n",
    "            # reasignación de grupo si es necesario\n",
    "            if (group_idx[i] != idx_min):\n",
    "                vec_reasig[i] == False\n",
    "                group_idx[i] = idx_min\n",
    "            # si no hay reasignación se pone en verdadero\n",
    "            else:\n",
    "                vec_reasig[i] = True\n",
    "    \n",
    "    for i in range(k):\n",
    "        group = x[group_idx == i]\n",
    "        # si hay algo en el grupo\n",
    "        if(group.shape[0] > 1):\n",
    "            stdev_vec[i] = np.std(group, axis = 0)\n",
    "    \n",
    "    print(np.histogram(group_idx,range(k+1)))\n",
    "            \n",
    "    return group_idx, stdev_vec, means_vec, cant_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_gaussian(x, c, mu):\n",
    "    # desvío fijo, igual al promedio entre el desvío de cada dimensión\n",
    "    mu = np.ones_like(mu) * np.mean(mu)\n",
    "    d_ = np.subtract(x, c)\n",
    "    c_ = np.diag(1 / mu)\n",
    "    # HAY QUE CHECKEAR LO DE LA DESVIACION POR QUE\n",
    "    #PONIENDOLA FIJA QUE ES LA SALVEDAD MAS SIMPLEMQUE DIJO DIPERSIA EL CODIGO ANDA.\n",
    "    y = np.exp(-0.5 * np.dot(np.dot(d_.T, 100), d_))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ds_regrecion(ds):\n",
    "    x_r=[]\n",
    "    y_r=[]\n",
    "    for i in range (ds.shape[0]-5):\n",
    "        x_r.append(ds[i:i+5])\n",
    "        y_r.append(ds[i+5])\n",
    "    return (x_r,y_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([29, 25, 48, 45, 44, 49, 33, 30, 35, 40], dtype=int64), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ds, testPartitionsIdx, trainPartitionsIdx  =  generar_particiones('gtp2datos/merval.csv', 1, 0.2)\n",
    "X,Yd=ds_regrecion(ds[trainPartitionsIdx][0])\n",
    "X=np.array(X)\n",
    "Yd=np.array(Yd).reshape((-1,1))\n",
    "k = 10        # cantidad de grupos / RBF\n",
    "# [puede ser que queden grupos vacíos?]\n",
    "idx_groups, std_vec, means_vec, cant_it = kMeans(X, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salidas de las RBF -> entradas del mlp\n",
    "mlp_in = np.zeros((X.shape[0], k))\n",
    "\n",
    "# por cada patrón, se calcula la salida de la capa radial pasando por la RBF\n",
    "for i, pattern in enumerate(X):\n",
    "    _aux = [f_gaussian(pattern, _mean, _mu) for (_mean, _mu) in zip(means_vec, std_vec)]\n",
    "    \n",
    "    mlp_in[i] = _aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(378, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#perceptrón\n",
    "layerStack = np.array([1])\n",
    "perceptron_simple = MultiLayerPerceptron(layerStack, k)\n",
    "\n",
    "# Parámetros de entrenamiento\n",
    "NEpoch = 500           # Cantidad de épocas máximas (anterior 2000)\n",
    "errorThr = 0.2         # Umbral de error para finalizar (anterior 0.005)\n",
    "lr = 5E-3                # Tasa de aprendizaje (anterior 8E-3)\n",
    "\n",
    "mlp_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 100: tasa de error de 0.9973544973544973 | MSE promedio 33980.62497473378\n",
      "Época 200: tasa de error de 0.9973544973544973 | MSE promedio 33980.62497473378\n",
      "Época 300: tasa de error de 0.9973544973544973 | MSE promedio 33980.62497473378\n",
      "Época 400: tasa de error de 0.9973544973544973 | MSE promedio 33980.62497473378\n",
      "Época 500: tasa de error de 0.9973544973544973 | MSE promedio 33980.62497473378\n",
      "Finalizó en la época 500 con una tasa de error de 0.9973544973544973\n"
     ]
    }
   ],
   "source": [
    "# Arreglos auxiliares para guardar la evolución del error de la red\n",
    "errorRatePerEpoch = []\n",
    "MMSEPerEpoch = []\n",
    "\n",
    "for epoch in range(NEpoch): # Para cada época\n",
    "    for pattern, yd in zip(mlp_in, Yd): # Para cada patrón en la partición\n",
    "        # Calcular la salida según los pesos actuales (pasada hacia adelante)\n",
    "        (_, y_) = perceptron_simple.eval(pattern)\n",
    "        #print(f'salida{_}={y_}')\n",
    "\n",
    "        # Realizar la propagación hacia atrás donde se calculan los gradientes\n",
    "        # instantáneos (pasada hacia atrás)\n",
    "        perceptron_simple.backward(y_, yd)\n",
    "        \n",
    "        # Actualizar los pesos de la red\n",
    "        perceptron_simple.update(y_, lr)\n",
    "    \n",
    "    # Para la validación se utilizarán solo algunos patrones y se calculará una\n",
    "    # tasa de error, si esta es menor al umbral, se termina el proceso de entrenamiento\n",
    "    # A la vez, se calculará el error cuadrático medio para tener una evolución\n",
    "    # de dicha variable a lo largo de las épocas\n",
    "    errorsAccum_ = 0    # Acumulador de errores\n",
    "    SEAcumm_ = 0        # Acumulador error cuadrático\n",
    "\n",
    "    # [valido con todos los patrones]\n",
    "    for patron, yd in zip(mlp_in, Yd):\n",
    "        # Evaluar el patron\n",
    "        (z_, _) = perceptron_simple.eval(patron)\n",
    "        #CODIFICACION\n",
    "        y_ = -1 if (z_[-1] < 0) else 1\n",
    "\n",
    "        # Comparación con la salida deseada y acumulación de errores\n",
    "        errorsAccum_ += int(np.any(np.not_equal(np.ceil(z_), yd)))\n",
    "  \n",
    "        # Cálculo del error cuadrático y acumulación\n",
    "        SEAcumm_ += np.sum(np.square(yd - z_))\n",
    "\n",
    "    # Tasa de error: errores / patrones evaluados\n",
    "    errorRate_ = (errorsAccum_/X.shape[0])\n",
    "    # Guardar la tasa de error de1000 la época\n",
    "    errorRatePerEpoch= np.append(errorRatePerEpoch, [errorRate_])\n",
    "    \n",
    "    # Calcular el error cuadrático medio promedio: MSE / patrones evaluados\n",
    "    MSEMean_= (SEAcumm_/X.shape[0])\n",
    "    # Guardar el error cuadrático medio promedio de la época\n",
    "    MMSEPerEpoch = np.append(MMSEPerEpoch, MSEMean_)\n",
    "\n",
    "    # Si la tasa de error es menor al umbral, termina el proceso de entrenamiento\n",
    "    if (errorRate_ < errorThr):\n",
    "        break\n",
    "\n",
    "    # Cada 100 épocas mostrar el error\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Época {epoch+1}: tasa de error de {errorRatePerEpoch[-1]} | MSE promedio {MMSEPerEpoch[-1]}')\n",
    "   \n",
    "# Imprimir información acerca del entrenamiento\n",
    "print(f'Finalizó en la época {epoch+1} con una tasa de error de {errorRatePerEpoch[-1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52634da84371cba311ea128a5ea7cdc41ff074b781779e754b270ff9f8153cee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
