{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, inputs: int, f_activacion: Callable[[float], float]):\n",
    "        \"\"\"\n",
    "        Parametros\n",
    "        ----------\n",
    "        inputs : `int`\n",
    "            Numero de entradas\n",
    "        f_activacion : `Callable[[float], float]`\n",
    "            Función de activación\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.f_activacion = f_activacion\n",
    "\n",
    "        # Inicialización de los pesos\n",
    "        # Se agrega un elemento para representar el bias\n",
    "        # y se resta 0.5 para que quede entre -0.5 y 0.5\n",
    "        self.w = np.random.rand(inputs + 1) - 0.5\n",
    "    \n",
    "    def eval(self, patron: np.ndarray) -> Tuple[np.ndarray, float, float]:\n",
    "        \"\"\"Evalua un patron y calcula el error como la diferencia entre\n",
    "        el valor deseado y el calculado\n",
    "\n",
    "        Parametros\n",
    "        ----------\n",
    "        patron : `ndarray`\n",
    "            Vector de entrada\n",
    "\n",
    "        Return\n",
    "        -------\n",
    "        `tuple`\n",
    "            [Salida calculada, Error, Patron evaluado (con bias)]\n",
    "        \"\"\"\n",
    "        x = np.hstack((-1, patron[:self.inputs]))\n",
    "        # Producto interno\n",
    "        z = np.inner(x, self.w)\n",
    "        # No linealidad\n",
    "        y = self.f_activacion(z)\n",
    "        # Error\n",
    "        err = patron[-1] - y\n",
    "        return (y, err, x)\n",
    "\n",
    "    def train(self, patron: np.ndarray, alpha: float) \\\n",
    "        -> Tuple[np.ndarray, float, float]:\n",
    "        \"\"\"Evalua un patron y a partir del error ajusta los pesos en base a la\n",
    "        tasa de aprendizaje alpha\n",
    "\n",
    "        Parametros\n",
    "        ----------\n",
    "        patron : `ndarray`\n",
    "            Vector de entrada\n",
    "        alpha : `float`\n",
    "            Tasa de aprendizaje\n",
    "\n",
    "        Return\n",
    "        -------\n",
    "        `tuple`\n",
    "            [Nuevos pesos, Salida calculada, Error]\n",
    "        \"\"\"\n",
    "        # Evaluación\n",
    "        (y, err, x) = self.eval(patron)\n",
    "        # Actualización de pesos\n",
    "        self.w = self.w + (alpha * err) * x\n",
    "        return (self.w, y, err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de activación \"signo\"\n",
    "def f_sign(a: np.number | np.ndarray) -> np.number | np.ndarray:\n",
    "    \"\"\"Función signo: Si `a < 0` devuelve `-1`, en caso contrario devuelve `1`\n",
    "    (vectorizable)\n",
    "    \"\"\"\n",
    "    return (np.heaviside(a, 1) * 2) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2\n",
    "\n",
    "Realice un programa que permita generar un conjunto de particiones de entrenamiento y prueba a partir de un único archivo de datos en formato texto separado por comas. El programa debe permitir seleccionar la cantidad de particiones y el porcentaje de patrones de entrenamiento y prueba. Para probarlo:\n",
    "\n",
    "1. El archivo `spheres1d10.csv` contiene una serie de datos generados a partir de los valores de la **Tabla 1**, con pequeñas desviaciones aleatorias ($< 10 \\%$) en torno a ellos (**Figura 2(a)**). Realice con estos datos la validación cruzada del perceptrón simple con 5 particiones de entrenamiento y prueba con relación 80/20.\n",
    "\n",
    "2. A partir de la misma tabla del ejemplo anterior, pero modificando el punto $x = [−1 + 1 − 1] \\to y_d = 1$, se ha generado un conjunto de datos diferente. Los archivos `spheres2d10.csv`, `spheres2d50.csv` y `spheres2d70.csv` contienen los datos con desviaciones aleatorias de $10$, $50$ y $70 \\%$, respectivamente (**Figuras 2(b)**, **2(c)** y **2(d)**). Realice la validación cruzada del perceptrón simple con 10 particiones de entrenamiento y prueba, con relación $80/20$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_partitions(filename: str, partitions: int, tr_rate: float, \\\n",
    "    CSV_delimiter : str = ',' ) -> Tuple[np.ndarray, np.ndarray] :\n",
    "    \"\"\"Crear particiones de entrenamiento y prueba\n",
    "    \n",
    "    A partir de un archivo CSV, genera un número dado de particiones\n",
    "    con una distribución de patrones de entrenamiento y prueba según el\n",
    "    `tr_rate` (en porcentaje). Se devuelven los patrones importados y \n",
    "    los índices correspondientes a los patrones que conforman cada una\n",
    "    de las particiones.\n",
    "\n",
    "    Parametros\n",
    "    ----------\n",
    "    filename : `str`\n",
    "        Nombre del archivo CSV\n",
    "    partitions : `int`\n",
    "        Cantidad de particiones\n",
    "    tr_rate : `float`\n",
    "        Porcentaje de patrones de entrenamiento\n",
    "\n",
    "    Return\n",
    "    -------\n",
    "    tuple\n",
    "        [Patrones, Partición de entrenamiento, Partición de prueba]\n",
    "    \"\"\"\n",
    "    # Cargar el archivo CSV\n",
    "    patterns = np.genfromtxt(filename, delimiter=CSV_delimiter)\n",
    "    num_patterns_ = patterns.shape[0]\n",
    "\n",
    "    # Se genera un listado de indices para cada partición mezclados en orden\n",
    "    # aleatorio\n",
    "    partitions_idx = np.array([ \\\n",
    "        np.random.choice(range(num_patterns_), num_patterns_, replace=False)\\\n",
    "            for i in range(partitions)])\n",
    "\n",
    "    # Se calcula el índice del corte entre patrones de train y test\n",
    "    cut_ = int(np.ceil(num_patterns_ * tr_rate))\n",
    "\n",
    "    # Al listado de indices para cada partición, se divide en el corte\n",
    "    partitions_splitted = np.split(partitions_idx, [cut_], axis=1)\n",
    "\n",
    "    # Se divide el dataset en train y test tomando los patrones según los índices\n",
    "    # de cada partición\n",
    "    # train = np.array([patterns[i,:] for i in partitions_splitted[0]])\n",
    "    # test = np.array([patterns[i,:] for i in partitions_splitted[1]])\n",
    "\n",
    "    return (patterns, partitions_splitted[0], partitions_splitted[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions = 5          # Cantidad de particiones\n",
    "partition_rate = 0.8    # Porcentaje de patrones para train\n",
    "(patterns, idx_train, idx_test) = \\\n",
    "    create_partitions('icgtp1datos/spheres1d10.csv', partitions, partition_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partición 0: Total de épocas = 19 - Error = 18.75 %\n",
      "Partición 0: Error con datos de prueba 58.75 %\n",
      "Partición 1: Total de épocas = 19 - Error = 8.75 %\n",
      "Partición 1: Error con datos de prueba 22.50 %\n",
      "Partición 2: Total de épocas = 19 - Error = 12.50 %\n",
      "Partición 2: Error con datos de prueba 28.75 %\n",
      "Partición 3: Total de épocas = 19 - Error = 17.50 %\n",
      "Partición 3: Error con datos de prueba 50.00 %\n",
      "Partición 4: Total de épocas = 19 - Error = 8.75 %\n",
      "Partición 4: Error con datos de prueba 20.00 %\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de patrones de train\n",
    "train_patterns = idx_train.shape[1]\n",
    "# Cantidad de patrones de test\n",
    "# test_patterns = idx_test.shape[1]\n",
    "\n",
    "# Cantidad de entradas\n",
    "inputs_number = patterns.shape[1]-1\n",
    "\n",
    "N = 20          # Nro de épocas\n",
    "alpha = 1E-4    # Tasa de aprendizaje\n",
    "nu = 0.1        # Umbral de error\n",
    "\n",
    "# Para calcular el error entre épocas se utilizará 10% de los patrones\n",
    "num_patterns_val = int(np.floor(train_patterns * 0.1))\n",
    "\n",
    "# Almacenar la tasa de error con datos de test de cada partición\n",
    "tst_error_rate = np.ndarray((partitions))\n",
    "\n",
    "for j in range(partitions):  # Para cada partición\n",
    "    # \"Separar\" los patrones de entrenamiento de la partición\n",
    "    partition = patterns[idx_train[j]]\n",
    "\n",
    "    # Iniciar el perceptrón simple\n",
    "    perceptron = Perceptron(inputs_number, f_sign)\n",
    "\n",
    "    # Entrenamiento por épocas\n",
    "    for i in range(N):                          # Para cada época\n",
    "        for pattern in partition:               # Para todos los patrones\n",
    "            perceptron.train(pattern, alpha)    # Entrenar\n",
    "    \n",
    "        # Para cada época se calcula el error\n",
    "        # Obtener índices aleatorios de los patrones que se utilizaran\n",
    "        # para la validación\n",
    "        val_patterns_idx = np.random.choice(range(partition.shape[0]), num_patterns_val, replace=False)\n",
    "        \n",
    "        errors = 0  # Acumulador de errores\n",
    "        for pattern in partition[val_patterns_idx]:  # Para cada patrón de validación\n",
    "            (y, _, _) = perceptron.eval(pattern)     # Evaluar\n",
    "            errors += int(y != pattern[-1])          # Contar si es un error o no\n",
    "        err_rate = errors / num_patterns_val         # Calcular la tasa de error\n",
    "\n",
    "        # Evolución del error por época\n",
    "        # print(f'Partición {j} - Época {i}: {err_rate * 100:.2f} %')\n",
    "\n",
    "        # Si el error calculado es menos del umbral cortar\n",
    "        if (err_rate < nu):\n",
    "            break\n",
    "\n",
    "    print(f'Partición {j}: Total de épocas = {i} - Error = {err_rate * 100:.2f} %')\n",
    "    \n",
    "    # Evaluar el perceptron con la partición de prueba\n",
    "    errors = 0  # Acumulador de errores\n",
    "    for patron in patterns[idx_test[j]]:        # Para cada patrón de la partición de test\n",
    "        (y, _, _) = perceptron.eval(patron)     # Evaluar\n",
    "        errors += int(y != patron[-1])          # Contar si es un error o no\n",
    "    err_rate = errors / num_patterns_val        # Calcular el error medio\n",
    "    tst_error_rate[j] = err_rate\n",
    "\n",
    "    print(f'Partición {j}: Error con datos de prueba {err_rate * 100:.2f} %')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
